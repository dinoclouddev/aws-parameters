Name, Allowed Values, Modifiable, Value, Source, Type, Minimum Version, Description
"activerehashing","yes,no",true,"yes","system","string","2.6.13","Apply rehashing or not."
"appendfsync","always,everysec,no",true,"everysec","system","string","2.6.13","fsync policy for AOF persistence"
"appendonly","yes,no",true,"no","system","string","2.6.13","Enable Redis persistence."
"client-output-buffer-limit-normal-hard-limit","0-",true,"0","system","integer","2.6.13","Normal client output buffer hard limit in bytes."
"client-output-buffer-limit-normal-soft-limit","0-",true,"0","system","integer","2.6.13","Normal client output buffer soft limit in bytes."
"client-output-buffer-limit-normal-soft-seconds","0-",true,"0","system","integer","2.6.13","Normal client output buffer soft limit in seconds."
"client-output-buffer-limit-pubsub-hard-limit","0-",true,"33554432","system","integer","2.6.13","Pubsub client output buffer hard limit in bytes."
"client-output-buffer-limit-pubsub-soft-limit","0-",true,"8388608","system","integer","2.6.13","Pubsub client output buffer soft limit in bytes."
"client-output-buffer-limit-pubsub-soft-seconds","0-",true,"60","system","integer","2.6.13","Pubsub client output buffer soft limit in seconds."
"client-output-buffer-limit-slave-soft-seconds","0-",false,"60","system","integer","2.6.13","Slave client output buffer soft limit in seconds."
"databases","1-1200000",true,"16","system","integer","2.6.13","Set the number of databases."
"hash-max-ziplist-entries","0-",true,"512","system","integer","2.6.13","The maximum number of hash entries in order for the dataset to be compressed."
"hash-max-ziplist-value","0-",true,"64","system","integer","2.6.13","The threshold of biggest hash entries in order for the dataset to be compressed."
"list-max-ziplist-entries","0-",true,"512","system","integer","2.6.13","The maximum number of list entries in order for the dataset to be compressed."
"list-max-ziplist-value","0-",true,"64","system","integer","2.6.13","The threshold of biggest list entries in order for the dataset to be compressed."
"lua-time-limit","5000",false,"5000","system","integer","2.6.13","Max execution time of a Lua script in milliseconds. 0 for unlimited execution without warnings."
"maxclients","1-65000",false,"65000","system","integer","2.6.13","The maximum number of Redis clients."
"maxmemory-policy","volatile-lru,allkeys-lru,volatile-random,allkeys-random,volatile-ttl,noeviction",true,"volatile-lru","system","string","2.6.13","Max memory policy."
"maxmemory-samples","1-",true,"3","system","integer","2.6.13","Max memory samples."
"reserved-memory","0-",true,"0","system","integer","2.6.13","The amount of memory reserved for non-cache memory usage, in bytes. You may want to increase this parameter for nodes with read replicas, AOF enabled, etc, to reduce swap usage."
"set-max-intset-entries","0-",true,"512","system","integer","2.6.13","The limit in the size of the set in order for the dataset to be compressed."
"slave-allow-chaining","yes,no",false,"no","system","string","2.6.13","Configures if chaining of slaves is allowed"
"slowlog-log-slower-than","-",true,"10000","system","integer","2.6.13","The execution time, in microseconds, to exceed in order for the command to get logged. Note that a negative number disables the slow log, while a value of zero forces the logging of every command."
"slowlog-max-len","0-",true,"128","system","integer","2.6.13","The length of the slow log. There is no limit to this length. Just be aware that it will consume memory. You can reclaim memory used by the slow log with SLOWLOG RESET."
"tcp-keepalive","0-",true,"0","system","integer","2.6.13","If non-zero, send ACKs every given number of seconds."
"timeout","0,20-",true,"0","system","integer","2.6.13","Close connection if client is idle for a given number of seconds, or never if 0."
"zset-max-ziplist-entries","0-",true,"128","system","integer","2.6.13","The maximum number of sorted set entries in order for the dataset to be compressed."
"zset-max-ziplist-value","0-",true,"64","system","integer","2.6.13","The threshold of biggest sorted set entries in order for the dataset to be compressed."
